{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da29503f",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-19T02:53:37.680510Z",
     "iopub.status.busy": "2025-12-19T02:53:37.680204Z",
     "iopub.status.idle": "2025-12-19T02:53:50.329871Z",
     "shell.execute_reply": "2025-12-19T02:53:50.328898Z"
    },
    "papermill": {
     "duration": 12.657245,
     "end_time": "2025-12-19T02:53:50.331189",
     "exception": false,
     "start_time": "2025-12-19T02:53:37.673944",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, confusion_matrix, roc_auc_score, \n",
    "                             roc_curve, classification_report)\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a923be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T02:53:50.340583Z",
     "iopub.status.busy": "2025-12-19T02:53:50.339797Z",
     "iopub.status.idle": "2025-12-19T02:53:50.345615Z",
     "shell.execute_reply": "2025-12-19T02:53:50.344943Z"
    },
    "papermill": {
     "duration": 0.01137,
     "end_time": "2025-12-19T02:53:50.346602",
     "exception": false,
     "start_time": "2025-12-19T02:53:50.335232",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    # Data paths\n",
    "    'data_dir': '/kaggle/input/140k-real-and-fake-faces',\n",
    "    'output_dir': '/kaggle/working',\n",
    "    \n",
    "    # Model parameters\n",
    "    'img_size': 224,\n",
    "    'batch_size': 64,\n",
    "    'num_epochs': 30,\n",
    "    'learning_rate': 0.0001,\n",
    "    'weight_decay': 1e-4,\n",
    "    \n",
    "    # Training parameters\n",
    "    'num_workers': 4,\n",
    "    'early_stopping_patience': 7,\n",
    "    'reduce_lr_patience': 3,\n",
    "    'reduce_lr_factor': 0.5,\n",
    "    \n",
    "    # Data augmentation\n",
    "    'rotation_degrees': 10,\n",
    "    'train_split': 0.8,\n",
    "    'val_split': 0.1,\n",
    "    # test_split will be 0.1\n",
    "    \n",
    "    # Seed\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "print(\"Hyperparameter Configuration:\")\n",
    "print(json.dumps(CONFIG, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5665b784",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T02:53:50.355049Z",
     "iopub.status.busy": "2025-12-19T02:53:50.354791Z",
     "iopub.status.idle": "2025-12-19T02:53:50.360710Z",
     "shell.execute_reply": "2025-12-19T02:53:50.359855Z"
    },
    "papermill": {
     "duration": 0.011429,
     "end_time": "2025-12-19T02:53:50.361782",
     "exception": false,
     "start_time": "2025-12-19T02:53:50.350353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FacePreprocessor:\n",
    "    \"\"\"Detect, crop, and preprocess faces from images\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Load face cascade classifier\n",
    "        cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
    "        self.face_cascade = cv2.CascadeClassifier(cascade_path)\n",
    "    \n",
    "    def detect_and_crop_face(self, image):\n",
    "        \"\"\"Detect face and crop to face region\"\"\"\n",
    "        # Convert PIL to numpy array\n",
    "        img_array = np.array(image)\n",
    "        \n",
    "        # Convert to grayscale for face detection\n",
    "        gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        # Detect faces\n",
    "        faces = self.face_cascade.detectMultiScale(\n",
    "            gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30)\n",
    "        )\n",
    "        \n",
    "        # If face detected, crop to largest face\n",
    "        if len(faces) > 0:\n",
    "            # Get largest face\n",
    "            largest_face = max(faces, key=lambda x: x[2] * x[3])\n",
    "            x, y, w, h = largest_face\n",
    "            \n",
    "            # Add padding (10%)\n",
    "            padding = int(0.1 * max(w, h))\n",
    "            x = max(0, x - padding)\n",
    "            y = max(0, y - padding)\n",
    "            w = w + 2 * padding\n",
    "            h = h + 2 * padding\n",
    "            \n",
    "            # Crop face\n",
    "            face_crop = img_array[y:y+h, x:x+w]\n",
    "            return Image.fromarray(face_crop)\n",
    "        \n",
    "        # If no face detected, return original\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52788955",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T02:53:50.370284Z",
     "iopub.status.busy": "2025-12-19T02:53:50.369774Z",
     "iopub.status.idle": "2025-12-19T02:53:50.374651Z",
     "shell.execute_reply": "2025-12-19T02:53:50.373986Z"
    },
    "papermill": {
     "duration": 0.010356,
     "end_time": "2025-12-19T02:53:50.375780",
     "exception": false,
     "start_time": "2025-12-19T02:53:50.365424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FaceDataset(Dataset):\n",
    "    \"\"\"Custom dataset for face classification with preprocessing\"\"\"\n",
    "    \n",
    "    def __init__(self, image_paths, labels, transform=None, preprocessor=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.preprocessor = preprocessor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Preprocess (crop face)\n",
    "        if self.preprocessor:\n",
    "            image = self.preprocessor.detect_and_crop_face(image)\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e4facd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T02:53:50.383980Z",
     "iopub.status.busy": "2025-12-19T02:53:50.383571Z",
     "iopub.status.idle": "2025-12-19T02:53:50.388656Z",
     "shell.execute_reply": "2025-12-19T02:53:50.387911Z"
    },
    "papermill": {
     "duration": 0.010302,
     "end_time": "2025-12-19T02:53:50.389659",
     "exception": false,
     "start_time": "2025-12-19T02:53:50.379357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_transforms(mode='train', img_size=224, rotation_degrees=10):\n",
    "    \"\"\"Get data transforms for training/validation/test\"\"\"\n",
    "    \n",
    "    if mode == 'train':\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.RandomRotation(degrees=rotation_degrees),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    else:\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b5045b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T02:53:50.397897Z",
     "iopub.status.busy": "2025-12-19T02:53:50.397681Z",
     "iopub.status.idle": "2025-12-19T02:53:52.864204Z",
     "shell.execute_reply": "2025-12-19T02:53:52.863292Z"
    },
    "papermill": {
     "duration": 2.472244,
     "end_time": "2025-12-19T02:53:52.865577",
     "exception": false,
     "start_time": "2025-12-19T02:53:50.393333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_dataset(data_dir, train_split=0.8, val_split=0.1):\n",
    "    \"\"\"Load and split dataset into train/val/test\"\"\"\n",
    "    \n",
    "    print(\"Loading dataset...\")\n",
    "    \n",
    "    # Get paths\n",
    "    real_dir = Path(data_dir) / 'real_vs_fake' / 'real-vs-fake' / 'train' / 'real'\n",
    "    fake_dir = Path(data_dir) / 'real_vs_fake' / 'real-vs-fake' / 'train' / 'fake'\n",
    "    \n",
    "    # Collect all image paths\n",
    "    real_images = list(real_dir.glob('*.jpg')) + list(real_dir.glob('*.png'))\n",
    "    fake_images = list(fake_dir.glob('*.jpg')) + list(fake_dir.glob('*.png'))\n",
    "    \n",
    "    print(f\"Found {len(real_images)} real images\")\n",
    "    print(f\"Found {len(fake_images)} fake images\")\n",
    "    \n",
    "    # Create labels (0: real, 1: fake)\n",
    "    real_labels = [0] * len(real_images)\n",
    "    fake_labels = [1] * len(fake_images)\n",
    "    \n",
    "    # Combine\n",
    "    all_images = real_images + fake_images\n",
    "    all_labels = real_labels + fake_labels\n",
    "    \n",
    "    # Shuffle\n",
    "    combined = list(zip(all_images, all_labels))\n",
    "    random.shuffle(combined)\n",
    "    all_images, all_labels = zip(*combined)\n",
    "    all_images = list(all_images)\n",
    "    all_labels = list(all_labels)\n",
    "    \n",
    "    # Split dataset\n",
    "    n_total = len(all_images)\n",
    "    n_train = int(train_split * n_total)\n",
    "    n_val = int(val_split * n_total)\n",
    "    \n",
    "    train_images = all_images[:n_train]\n",
    "    train_labels = all_labels[:n_train]\n",
    "    \n",
    "    val_images = all_images[n_train:n_train+n_val]\n",
    "    val_labels = all_labels[n_train:n_train+n_val]\n",
    "    \n",
    "    test_images = all_images[n_train+n_val:]\n",
    "    test_labels = all_labels[n_train+n_val:]\n",
    "    \n",
    "    print(f\"\\nDataset split:\")\n",
    "    print(f\"Train: {len(train_images)} images\")\n",
    "    print(f\"Val: {len(val_images)} images\")\n",
    "    print(f\"Test: {len(test_images)} images\")\n",
    "    \n",
    "    return (train_images, train_labels), (val_images, val_labels), (test_images, test_labels)\n",
    "\n",
    "# Load dataset\n",
    "train_data, val_data, test_data = load_dataset(\n",
    "    CONFIG['data_dir'], \n",
    "    CONFIG['train_split'], \n",
    "    CONFIG['val_split']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cb960f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T02:53:52.874531Z",
     "iopub.status.busy": "2025-12-19T02:53:52.874312Z",
     "iopub.status.idle": "2025-12-19T02:53:52.908649Z",
     "shell.execute_reply": "2025-12-19T02:53:52.907770Z"
    },
    "papermill": {
     "duration": 0.040004,
     "end_time": "2025-12-19T02:53:52.909793",
     "exception": false,
     "start_time": "2025-12-19T02:53:52.869789",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\nCreating dataloaders...\")\n",
    "\n",
    "preprocessor = FacePreprocessor()\n",
    "\n",
    "train_dataset = FaceDataset(\n",
    "    train_data[0], train_data[1],\n",
    "    transform=get_transforms('train', CONFIG['img_size'], CONFIG['rotation_degrees']),\n",
    "    preprocessor=preprocessor\n",
    ")\n",
    "\n",
    "val_dataset = FaceDataset(\n",
    "    val_data[0], val_data[1],\n",
    "    transform=get_transforms('val', CONFIG['img_size']),\n",
    "    preprocessor=preprocessor\n",
    ")\n",
    "\n",
    "test_dataset = FaceDataset(\n",
    "    test_data[0], test_data[1],\n",
    "    transform=get_transforms('test', CONFIG['img_size']),\n",
    "    preprocessor=preprocessor\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=CONFIG['num_workers'],\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=CONFIG['num_workers'],\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=CONFIG['num_workers'],\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda1e7a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T02:53:52.918661Z",
     "iopub.status.busy": "2025-12-19T02:53:52.918451Z",
     "iopub.status.idle": "2025-12-19T02:53:53.124798Z",
     "shell.execute_reply": "2025-12-19T02:53:53.123608Z"
    },
    "papermill": {
     "duration": 0.212422,
     "end_time": "2025-12-19T02:53:53.126205",
     "exception": false,
     "start_time": "2025-12-19T02:53:52.913783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FaceClassifierCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom CNN for face classification\n",
    "    Architecture: 5 convolutional blocks + 3 fully connected layers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=2, dropout_rate=0.5):\n",
    "        super(FaceClassifierCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional Block 1\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(0.2)\n",
    "        )\n",
    "        \n",
    "        # Convolutional Block 2\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(0.3)\n",
    "        )\n",
    "        \n",
    "        # Convolutional Block 3\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(0.4)\n",
    "        )\n",
    "        \n",
    "        # Convolutional Block 4\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(0.4)\n",
    "        )\n",
    "        \n",
    "        # Convolutional Block 5\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(0.5)\n",
    "        )\n",
    "        \n",
    "        # Global Average Pooling\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "model = FaceClassifierCNN(num_classes=2, dropout_rate=0.5)\n",
    "model = model.to(device)\n",
    "\n",
    "# Multi-GPU support\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(model)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091eee8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T02:53:53.135611Z",
     "iopub.status.busy": "2025-12-19T02:53:53.135401Z",
     "iopub.status.idle": "2025-12-19T02:53:53.140871Z",
     "shell.execute_reply": "2025-12-19T02:53:53.140215Z"
    },
    "papermill": {
     "duration": 0.011206,
     "end_time": "2025-12-19T02:53:53.141830",
     "exception": false,
     "start_time": "2025-12-19T02:53:53.130624",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loss function: Cross Entropy Loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer: Adam with weight decay\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG['learning_rate'],\n",
    "    weight_decay=CONFIG['weight_decay']\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=CONFIG['reduce_lr_factor'],\n",
    "    patience=CONFIG['reduce_lr_patience'],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\nObjective and Loss Functions:\")\n",
    "print(f\"Loss Function: Cross Entropy Loss\")\n",
    "print(f\"Optimizer: Adam (lr={CONFIG['learning_rate']}, weight_decay={CONFIG['weight_decay']})\")\n",
    "print(f\"Scheduler: ReduceLROnPlateau (patience={CONFIG['reduce_lr_patience']}, factor={CONFIG['reduce_lr_factor']})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ed72c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T02:53:53.151213Z",
     "iopub.status.busy": "2025-12-19T02:53:53.150990Z",
     "iopub.status.idle": "2025-12-19T02:53:53.158794Z",
     "shell.execute_reply": "2025-12-19T02:53:53.158212Z"
    },
    "papermill": {
     "duration": 0.013945,
     "end_time": "2025-12-19T02:53:53.159824",
     "exception": false,
     "start_time": "2025-12-19T02:53:53.145879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc='Training')\n",
    "    for inputs, labels in pbar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{100.*correct/total:.2f}%'\n",
    "        })\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate the model\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader, desc='Validation'):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs[:, 1].cpu().numpy())\n",
    "    \n",
    "    epoch_loss = running_loss / len(val_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc, all_preds, all_labels, all_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5e2451",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T02:53:53.169405Z",
     "iopub.status.busy": "2025-12-19T02:53:53.169208Z",
     "iopub.status.idle": "2025-12-19T14:29:16.298995Z",
     "shell.execute_reply": "2025-12-19T14:29:16.297888Z"
    },
    "papermill": {
     "duration": 41723.136378,
     "end_time": "2025-12-19T14:29:16.300486",
     "exception": false,
     "start_time": "2025-12-19T02:53:53.164108",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize logging\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': [],\n",
    "    'learning_rates': []\n",
    "}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_val_acc = 0.0\n",
    "patience_counter = 0\n",
    "start_time = time.time()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(CONFIG['num_epochs']):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}/{CONFIG['num_epochs']}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc, val_preds, val_labels, val_probs = validate(\n",
    "        model, val_loader, criterion, device\n",
    "    )\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step(val_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Log metrics\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['learning_rates'].append(current_lr)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    print(f\"\\nEpoch Summary:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "    print(f\"  Learning Rate: {current_lr:.6f}\")\n",
    "    print(f\"  Time: {epoch_time:.2f}s\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # Save model\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc,\n",
    "            'config': CONFIG\n",
    "        }, os.path.join(CONFIG['output_dir'], 'best_model.pth'))\n",
    "        \n",
    "        print(f\"  ✓ New best model saved! (Val Loss: {val_loss:.4f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"  No improvement. Patience: {patience_counter}/{CONFIG['early_stopping_patience']}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= CONFIG['early_stopping_patience']:\n",
    "        print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
    "        break\n",
    "    \n",
    "    # Check time limit (11.5 hours to be safe)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    if elapsed_time > 11.5 * 3600:\n",
    "        print(f\"\\nApproaching time limit. Stopping training.\")\n",
    "        break\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"TRAINING COMPLETED\")\n",
    "print(f\"Total time: {total_time/3600:.2f} hours\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Save training history\n",
    "with open(os.path.join(CONFIG['output_dir'], 'training_history.json'), 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af36eaf3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T14:29:22.222121Z",
     "iopub.status.busy": "2025-12-19T14:29:22.221331Z",
     "iopub.status.idle": "2025-12-19T14:29:22.278238Z",
     "shell.execute_reply": "2025-12-19T14:29:22.277443Z"
    },
    "papermill": {
     "duration": 2.941636,
     "end_time": "2025-12-19T14:29:22.279329",
     "exception": false,
     "start_time": "2025-12-19T14:29:19.337693",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Loading best model for evaluation...\")\n",
    "checkpoint = torch.load(os.path.join(CONFIG['output_dir'], 'best_model.pth'))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Loaded model from epoch {checkpoint['epoch']+1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32baddff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T14:29:28.209105Z",
     "iopub.status.busy": "2025-12-19T14:29:28.208807Z",
     "iopub.status.idle": "2025-12-19T14:32:29.480273Z",
     "shell.execute_reply": "2025-12-19T14:32:29.479407Z"
    },
    "papermill": {
     "duration": 184.186078,
     "end_time": "2025-12-19T14:32:29.481404",
     "exception": false,
     "start_time": "2025-12-19T14:29:25.295326",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATING ON TEST SET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "model.eval()\n",
    "test_preds = []\n",
    "test_labels = []\n",
    "test_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(test_loader, desc='Testing'):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        _, predicted = outputs.max(1)\n",
    "        \n",
    "        test_preds.extend(predicted.cpu().numpy())\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "        test_probs.extend(probs[:, 1].cpu().numpy())\n",
    "\n",
    "test_preds = np.array(test_preds)\n",
    "test_labels = np.array(test_labels)\n",
    "test_probs = np.array(test_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944852d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T14:32:35.509508Z",
     "iopub.status.busy": "2025-12-19T14:32:35.509226Z",
     "iopub.status.idle": "2025-12-19T14:32:35.553185Z",
     "shell.execute_reply": "2025-12-19T14:32:35.552408Z"
    },
    "papermill": {
     "duration": 2.97732,
     "end_time": "2025-12-19T14:32:35.554351",
     "exception": false,
     "start_time": "2025-12-19T14:32:32.577031",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"1. EVALUATION METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(test_labels, test_preds)\n",
    "precision = precision_score(test_labels, test_preds, average='binary')\n",
    "recall = recall_score(test_labels, test_preds, average='binary')\n",
    "f1 = f1_score(test_labels, test_preds, average='binary')\n",
    "auc_score = roc_auc_score(test_labels, test_probs)\n",
    "\n",
    "print(f\"\\nTest Set Performance:\")\n",
    "print(f\"  Accuracy:  {accuracy*100:.2f}%\")\n",
    "print(f\"  Precision: {precision*100:.2f}%\")\n",
    "print(f\"  Recall:    {recall*100:.2f}%\")\n",
    "print(f\"  F1-Score:  {f1*100:.2f}%\")\n",
    "print(f\"  AUC-ROC:   {auc_score:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_labels, test_preds, \n",
    "                          target_names=['Real', 'Fake']))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afc7d58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T14:32:41.496327Z",
     "iopub.status.busy": "2025-12-19T14:32:41.496045Z",
     "iopub.status.idle": "2025-12-19T14:32:41.502731Z",
     "shell.execute_reply": "2025-12-19T14:32:41.501992Z"
    },
    "papermill": {
     "duration": 2.962521,
     "end_time": "2025-12-19T14:32:41.503893",
     "exception": false,
     "start_time": "2025-12-19T14:32:38.541372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"2. EXPERIMENTAL METHODOLOGIES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "methodology_report = f\"\"\"\n",
    "Dataset:\n",
    "  - Source: Kaggle 140k Real and Fake Faces (StyleGAN generated fakes)\n",
    "  - Total Images: {len(train_data[0]) + len(val_data[0]) + len(test_data[0])}\n",
    "  - Train Split: {CONFIG['train_split']*100}% ({len(train_data[0])} images)\n",
    "  - Validation Split: {CONFIG['val_split']*100}% ({len(val_data[0])} images)\n",
    "  - Test Split: {(1-CONFIG['train_split']-CONFIG['val_split'])*100}% ({len(test_data[0])} images)\n",
    "\n",
    "Preprocessing Pipeline:\n",
    "  1. Face Detection: Haar Cascade Classifier\n",
    "  2. Face Cropping: Extract face region with 10% padding\n",
    "  3. Image Rotation: Random rotation up to ±{CONFIG['rotation_degrees']}°\n",
    "  4. Resize: {CONFIG['img_size']}x{CONFIG['img_size']} pixels\n",
    "  5. Normalization: ImageNet statistics\n",
    "\n",
    "Data Augmentation (Training only):\n",
    "  - Random horizontal flip (p=0.5)\n",
    "  - Random rotation (±{CONFIG['rotation_degrees']}°)\n",
    "  - Color jitter (brightness, contrast, saturation ±0.2)\n",
    "\n",
    "Model Architecture:\n",
    "  - Type: Custom Convolutional Neural Network\n",
    "  - Layers: 5 convolutional blocks + 3 FC layers\n",
    "  - Parameters: {trainable_params:,}\n",
    "  - Regularization: Batch normalization, dropout (0.2-0.5)\n",
    "  - Activation: ReLU\n",
    "  - Pooling: MaxPool2d + Global Average Pooling\n",
    "\n",
    "Training Configuration:\n",
    "  - Batch Size: {CONFIG['batch_size']}\n",
    "  - Epochs: {CONFIG['num_epochs']} (early stopping enabled)\n",
    "  - Optimizer: Adam\n",
    "  - Learning Rate: {CONFIG['learning_rate']} (adaptive)\n",
    "  - Weight Decay: {CONFIG['weight_decay']}\n",
    "  - Loss Function: Cross Entropy\n",
    "  - Device: {device} ({torch.cuda.device_count()} GPU(s))\n",
    "\n",
    "Validation Strategy:\n",
    "  - Early Stopping: Patience {CONFIG['early_stopping_patience']} epochs\n",
    "  - LR Scheduler: ReduceLROnPlateau (patience {CONFIG['reduce_lr_patience']})\n",
    "  - Best Model Selection: Lowest validation loss\n",
    "\"\"\"\n",
    "\n",
    "print(methodology_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe4bb14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T14:32:47.480952Z",
     "iopub.status.busy": "2025-12-19T14:32:47.480278Z",
     "iopub.status.idle": "2025-12-19T14:32:47.487187Z",
     "shell.execute_reply": "2025-12-19T14:32:47.486341Z"
    },
    "papermill": {
     "duration": 2.933017,
     "end_time": "2025-12-19T14:32:47.488498",
     "exception": false,
     "start_time": "2025-12-19T14:32:44.555481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"3. EXPERIMENTAL RESULTS AND PERFORMANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results_analysis = f\"\"\"\n",
    "Training Performance:\n",
    "  - Final Training Accuracy: {history['train_acc'][-1]:.2f}%\n",
    "  - Final Training Loss: {history['train_loss'][-1]:.4f}\n",
    "  - Best Validation Accuracy: {best_val_acc:.2f}%\n",
    "  - Best Validation Loss: {best_val_loss:.4f}\n",
    "  - Training Time: {total_time/3600:.2f} hours\n",
    "  - Epochs Completed: {len(history['train_loss'])}\n",
    "\n",
    "Test Set Performance:\n",
    "  - Test Accuracy: {accuracy*100:.2f}%\n",
    "  - Test Precision: {precision*100:.2f}%\n",
    "  - Test Recall: {recall*100:.2f}%\n",
    "  - Test F1-Score: {f1*100:.2f}%\n",
    "  - AUC-ROC Score: {auc_score:.4f}\n",
    "\n",
    "Class-wise Performance:\n",
    "  Real Images (Class 0):\n",
    "    - True Positives: {cm[0,0]}\n",
    "    - False Negatives: {cm[0,1]}\n",
    "    - Precision: {cm[0,0]/(cm[0,0]+cm[1,0])*100:.2f}%\n",
    "    - Recall: {cm[0,0]/(cm[0,0]+cm[0,1])*100:.2f}%\n",
    "  \n",
    "  Fake Images (Class 1):\n",
    "    - True Positives: {cm[1,1]}\n",
    "    - False Negatives: {cm[1,0]}\n",
    "    - Precision: {cm[1,1]/(cm[1,1]+cm[0,1])*100:.2f}%\n",
    "    - Recall: {cm[1,1]/(cm[1,1]+cm[1,0])*100:.2f}%\n",
    "\n",
    "Model Generalization:\n",
    "  - Train-Val Gap: {abs(history['train_acc'][-1] - history['val_acc'][-1]):.2f}%\n",
    "  - Val-Test Gap: {abs(best_val_acc - accuracy*100):.2f}%\n",
    "  - Overfitting Assessment: {'Minimal' if abs(history['train_acc'][-1] - accuracy*100) < 5 else 'Moderate' if abs(history['train_acc'][-1] - accuracy*100) < 10 else 'Significant'}\n",
    "\"\"\"\n",
    "\n",
    "print(results_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc633fe7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T14:32:53.392976Z",
     "iopub.status.busy": "2025-12-19T14:32:53.392243Z",
     "iopub.status.idle": "2025-12-19T14:32:53.398047Z",
     "shell.execute_reply": "2025-12-19T14:32:53.397262Z"
    },
    "papermill": {
     "duration": 2.934923,
     "end_time": "2025-12-19T14:32:53.399314",
     "exception": false,
     "start_time": "2025-12-19T14:32:50.464391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\nGenerating visualizations...\")\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542b328c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T14:32:59.361233Z",
     "iopub.status.busy": "2025-12-19T14:32:59.360473Z",
     "iopub.status.idle": "2025-12-19T14:33:02.537437Z",
     "shell.execute_reply": "2025-12-19T14:33:02.536611Z"
    },
    "papermill": {
     "duration": 6.107696,
     "end_time": "2025-12-19T14:33:02.540230",
     "exception": false,
     "start_time": "2025-12-19T14:32:56.432534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Loss Curves\n",
    "axes[0, 0].plot(history['train_loss'], label='Train Loss', linewidth=2, marker='o')\n",
    "axes[0, 0].plot(history['val_loss'], label='Val Loss', linewidth=2, marker='s')\n",
    "axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0, 0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend(fontsize=10)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Accuracy Curves\n",
    "axes[0, 1].plot(history['train_acc'], label='Train Accuracy', linewidth=2, marker='o')\n",
    "axes[0, 1].plot(history['val_acc'], label='Val Accuracy', linewidth=2, marker='s')\n",
    "axes[0, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[0, 1].set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend(fontsize=10)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Learning Rate Schedule\n",
    "axes[1, 0].plot(history['learning_rates'], linewidth=2, marker='o', color='red')\n",
    "axes[1, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Learning Rate', fontsize=12)\n",
    "axes[1, 0].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_yscale('log')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Train-Val Loss Comparison\n",
    "epochs = range(1, len(history['train_loss']) + 1)\n",
    "axes[1, 1].fill_between(epochs, history['train_loss'], history['val_loss'], \n",
    "                         alpha=0.3, label='Loss Gap')\n",
    "axes[1, 1].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[1, 1].plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Loss', fontsize=12)\n",
    "axes[1, 1].set_title('Generalization Gap Analysis', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].legend(fontsize=10)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG['output_dir'], 'learning_curves.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Learning curves saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c443ab76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T14:33:08.520762Z",
     "iopub.status.busy": "2025-12-19T14:33:08.520036Z",
     "iopub.status.idle": "2025-12-19T14:33:09.235591Z",
     "shell.execute_reply": "2025-12-19T14:33:09.234838Z"
    },
    "papermill": {
     "duration": 3.627485,
     "end_time": "2025-12-19T14:33:09.236720",
     "exception": false,
     "start_time": "2025-12-19T14:33:05.609235",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,\n",
    "            xticklabels=['Real', 'Fake'],\n",
    "            yticklabels=['Real', 'Fake'],\n",
    "            ax=ax, annot_kws={'size': 16})\n",
    "ax.set_xlabel('Predicted Label', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('True Label', fontsize=14, fontweight='bold')\n",
    "ax.set_title('Confusion Matrix - Test Set', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Add percentages\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        percentage = cm[i, j] / cm[i].sum() * 100\n",
    "        ax.text(j + 0.5, i + 0.7, f'({percentage:.1f}%)', \n",
    "                ha='center', va='center', fontsize=12, color='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG['output_dir'], 'confusion_matrix.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Confusion matrix saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c785084b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T14:33:15.199644Z",
     "iopub.status.busy": "2025-12-19T14:33:15.198908Z",
     "iopub.status.idle": "2025-12-19T14:33:16.056473Z",
     "shell.execute_reply": "2025-12-19T14:33:16.055780Z"
    },
    "papermill": {
     "duration": 3.906799,
     "end_time": "2025-12-19T14:33:16.058176",
     "exception": false,
     "start_time": "2025-12-19T14:33:12.151377",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(test_labels, test_probs)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.plot(fpr, tpr, linewidth=3, label=f'ROC Curve (AUC = {auc_score:.4f})')\n",
    "ax.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier')\n",
    "ax.set_xlabel('False Positive Rate', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('True Positive Rate', fontsize=14, fontweight='bold')\n",
    "ax.set_title('ROC Curve - Test Set', fontsize=16, fontweight='bold')\n",
    "ax.legend(fontsize=12, loc='lower right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG['output_dir'], 'roc_curve.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"ROC curve saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a42b4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T14:33:21.901086Z",
     "iopub.status.busy": "2025-12-19T14:33:21.900571Z",
     "iopub.status.idle": "2025-12-19T14:33:22.474329Z",
     "shell.execute_reply": "2025-12-19T14:33:22.473560Z"
    },
    "papermill": {
     "duration": 3.470482,
     "end_time": "2025-12-19T14:33:22.475675",
     "exception": false,
     "start_time": "2025-12-19T14:33:19.005193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "metrics_values = [accuracy * 100, precision * 100, recall * 100, f1 * 100]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars = ax.bar(metrics_names, metrics_values, color=['#3498db', '#2ecc71', '#e74c3c', '#f39c12'], \n",
    "              alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, metrics_values):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{value:.2f}%',\n",
    "            ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Score (%)', fontsize=14, fontweight='bold')\n",
    "ax.set_title('Test Set Performance Metrics', fontsize=16, fontweight='bold')\n",
    "ax.set_ylim([0, 105])\n",
    "ax.grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG['output_dir'], 'performance_metrics.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Performance metrics chart saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cb85f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T14:33:28.498308Z",
     "iopub.status.busy": "2025-12-19T14:33:28.498030Z",
     "iopub.status.idle": "2025-12-19T14:33:28.510887Z",
     "shell.execute_reply": "2025-12-19T14:33:28.510020Z"
    },
    "papermill": {
     "duration": 2.952757,
     "end_time": "2025-12-19T14:33:28.512455",
     "exception": false,
     "start_time": "2025-12-19T14:33:25.559698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SUMMARY REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary_report = f\"\"\"\n",
    "╔══════════════════════════════════════════════════════════════════════════════╗\n",
    "║                    AI-GENERATED FACE DETECTION MODEL                         ║\n",
    "║                         Training Summary Report                              ║\n",
    "╚══════════════════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "1. EVALUATION METRICS\n",
    "{'='*80}\n",
    "   Test Set Performance:\n",
    "   ├─ Accuracy:  {accuracy*100:.2f}%\n",
    "   ├─ Precision: {precision*100:.2f}%\n",
    "   ├─ Recall:    {recall*100:.2f}%\n",
    "   ├─ F1-Score:  {f1*100:.2f}%\n",
    "   └─ AUC-ROC:   {auc_score:.4f}\n",
    "\n",
    "   Confusion Matrix:\n",
    "   ┌─────────────┬──────────┬──────────┐\n",
    "   │             │ Pred: Real│ Pred: Fake│\n",
    "   ├─────────────┼──────────┼──────────┤\n",
    "   │ True: Real  │   {cm[0,0]:5d}  │   {cm[0,1]:5d}  │\n",
    "   │ True: Fake  │   {cm[1,0]:5d}  │   {cm[1,1]:5d}  │\n",
    "   └─────────────┴──────────┴──────────┘\n",
    "\n",
    "2. EXPERIMENTAL METHODOLOGIES\n",
    "{'='*80}\n",
    "   Dataset Configuration:\n",
    "   ├─ Total Samples: {len(train_data[0]) + len(val_data[0]) + len(test_data[0]):,}\n",
    "   ├─ Train: {len(train_data[0]):,} ({CONFIG['train_split']*100:.0f}%)\n",
    "   ├─ Validation: {len(val_data[0]):,} ({CONFIG['val_split']*100:.0f}%)\n",
    "   └─ Test: {len(test_data[0]):,} ({(1-CONFIG['train_split']-CONFIG['val_split'])*100:.0f}%)\n",
    "\n",
    "   Preprocessing Pipeline:\n",
    "   ├─ Face Detection: Haar Cascade\n",
    "   ├─ Face Cropping: Auto with 10% padding\n",
    "   ├─ Rotation: ±{CONFIG['rotation_degrees']}°\n",
    "   ├─ Resize: {CONFIG['img_size']}x{CONFIG['img_size']}\n",
    "   └─ Normalization: ImageNet statistics\n",
    "\n",
    "   Model Architecture:\n",
    "   ├─ Type: Custom CNN (5 conv blocks + 3 FC)\n",
    "   ├─ Total Parameters: {total_params:,}\n",
    "   ├─ Trainable Parameters: {trainable_params:,}\n",
    "   └─ Regularization: BatchNorm + Dropout\n",
    "\n",
    "3. EXPERIMENTAL RESULTS AND PERFORMANCE ANALYSIS\n",
    "{'='*80}\n",
    "   Training Dynamics:\n",
    "   ├─ Epochs Completed: {len(history['train_loss'])}\n",
    "   ├─ Training Time: {total_time/3600:.2f} hours\n",
    "   ├─ Final Train Accuracy: {history['train_acc'][-1]:.2f}%\n",
    "   ├─ Best Val Accuracy: {best_val_acc:.2f}%\n",
    "   └─ Test Accuracy: {accuracy*100:.2f}%\n",
    "\n",
    "   Generalization Analysis:\n",
    "   ├─ Train-Val Gap: {abs(history['train_acc'][-1] - history['val_acc'][-1]):.2f}%\n",
    "   ├─ Val-Test Gap: {abs(best_val_acc - accuracy*100):.2f}%\n",
    "   └─ Overfitting: {'Minimal ✓' if abs(history['train_acc'][-1] - accuracy*100) < 5 else 'Moderate ⚠' if abs(history['train_acc'][-1] - accuracy*100) < 10 else 'Significant ✗'}\n",
    "\n",
    "   Class Performance:\n",
    "   ├─ Real Images:\n",
    "   │  ├─ Precision: {cm[0,0]/(cm[0,0]+cm[1,0])*100:.2f}%\n",
    "   │  └─ Recall: {cm[0,0]/(cm[0,0]+cm[0,1])*100:.2f}%\n",
    "   └─ Fake Images:\n",
    "      ├─ Precision: {cm[1,1]/(cm[1,1]+cm[0,1])*100:.2f}%\n",
    "      └─ Recall: {cm[1,1]/(cm[1,1]+cm[1,0])*100:.2f}%\n",
    "\n",
    "4. OBJECTIVE AND LOSS FUNCTIONS\n",
    "{'='*80}\n",
    "   Loss Function: Cross Entropy Loss\n",
    "   ├─ Purpose: Multi-class classification\n",
    "   ├─ Formula: L = -Σ y_i * log(ŷ_i)\n",
    "   └─ Properties: Penalizes confident wrong predictions\n",
    "\n",
    "   Optimizer: Adam (Adaptive Moment Estimation)\n",
    "   ├─ Learning Rate: {CONFIG['learning_rate']}\n",
    "   ├─ Weight Decay: {CONFIG['weight_decay']} (L2 regularization)\n",
    "   ├─ Beta1: 0.9 (default)\n",
    "   └─ Beta2: 0.999 (default)\n",
    "\n",
    "   Learning Rate Scheduler:\n",
    "   ├─ Type: ReduceLROnPlateau\n",
    "   ├─ Monitor: Validation Loss\n",
    "   ├─ Patience: {CONFIG['reduce_lr_patience']} epochs\n",
    "   ├─ Reduction Factor: {CONFIG['reduce_lr_factor']}\n",
    "   └─ Final LR: {history['learning_rates'][-1]:.6f}\n",
    "\n",
    "5. LEARNING CURVES\n",
    "{'='*80}\n",
    "   Training Loss Progression:\n",
    "   ├─ Initial: {history['train_loss'][0]:.4f}\n",
    "   ├─ Final: {history['train_loss'][-1]:.4f}\n",
    "   └─ Reduction: {(1 - history['train_loss'][-1]/history['train_loss'][0])*100:.1f}%\n",
    "\n",
    "   Validation Loss Progression:\n",
    "   ├─ Initial: {history['val_loss'][0]:.4f}\n",
    "   ├─ Best: {best_val_loss:.4f}\n",
    "   └─ Final: {history['val_loss'][-1]:.4f}\n",
    "\n",
    "   Accuracy Progression:\n",
    "   ├─ Train: {history['train_acc'][0]:.2f}% → {history['train_acc'][-1]:.2f}%\n",
    "   ├─ Val: {history['val_acc'][0]:.2f}% → {history['val_acc'][-1]:.2f}%\n",
    "   └─ Test: {accuracy*100:.2f}%\n",
    "\n",
    "6. HYPERPARAMETER CONFIGURATION\n",
    "{'='*80}\n",
    "   Model Hyperparameters:\n",
    "   ├─ Input Size: {CONFIG['img_size']}x{CONFIG['img_size']}x3\n",
    "   ├─ Dropout Rate: 0.5\n",
    "   ├─ Batch Normalization: Enabled\n",
    "   └─ Activation: ReLU\n",
    "\n",
    "   Training Hyperparameters:\n",
    "   ├─ Batch Size: {CONFIG['batch_size']}\n",
    "   ├─ Max Epochs: {CONFIG['num_epochs']}\n",
    "   ├─ Initial Learning Rate: {CONFIG['learning_rate']}\n",
    "   ├─ Weight Decay: {CONFIG['weight_decay']}\n",
    "   ├─ Early Stopping Patience: {CONFIG['early_stopping_patience']}\n",
    "   └─ LR Reduction Patience: {CONFIG['reduce_lr_patience']}\n",
    "\n",
    "   Data Augmentation:\n",
    "   ├─ Random Rotation: ±{CONFIG['rotation_degrees']}°\n",
    "   ├─ Horizontal Flip: 50% probability\n",
    "   ├─ Color Jitter: ±0.2\n",
    "   └─ Applied to: Training set only\n",
    "\n",
    "   Computational Resources:\n",
    "   ├─ Device: {device}\n",
    "   ├─ GPUs: {torch.cuda.device_count()}\n",
    "   ├─ GPU Model: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A'}\n",
    "   └─ Time Limit: 12 hours (Kaggle)\n",
    "\n",
    "╔══════════════════════════════════════════════════════════════════════════════╗\n",
    "║                           TRAINING COMPLETED                                 ║\n",
    "║                                                                              ║\n",
    "║  All models, logs, and visualizations saved to: {CONFIG['output_dir']}\n",
    "║                                                                              ║\n",
    "║  Generated Files:                                                            ║\n",
    "║  ├─ best_model.pth           (Best model checkpoint)                        ║\n",
    "║  ├─ training_history.json    (Training logs)                                ║\n",
    "║  ├─ learning_curves.png      (Loss & accuracy plots)                        ║\n",
    "║  ├─ confusion_matrix.png     (Confusion matrix heatmap)                     ║\n",
    "║  ├─ roc_curve.png            (ROC curve)                                    ║\n",
    "║  └─ performance_metrics.png  (Bar chart of metrics)                         ║\n",
    "╚══════════════════════════════════════════════════════════════════════════════╝\n",
    "\"\"\"\n",
    "\n",
    "print(summary_report)\n",
    "\n",
    "# Save summary report\n",
    "with open(os.path.join(CONFIG['output_dir'], 'summary_report.txt'), 'w') as f:\n",
    "    f.write(summary_report)\n",
    "\n",
    "print(\"\\n✓ Summary report saved to summary_report.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b28527b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T14:33:34.468280Z",
     "iopub.status.busy": "2025-12-19T14:33:34.468005Z",
     "iopub.status.idle": "2025-12-19T14:33:34.500597Z",
     "shell.execute_reply": "2025-12-19T14:33:34.499905Z"
    },
    "papermill": {
     "duration": 2.921937,
     "end_time": "2025-12-19T14:33:34.501810",
     "exception": false,
     "start_time": "2025-12-19T14:33:31.579873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\nSaving predictions...\")\n",
    "\n",
    "# Create predictions dataframe\n",
    "predictions_df = pd.DataFrame({\n",
    "    'true_label': test_labels,\n",
    "    'predicted_label': test_preds,\n",
    "    'probability_fake': test_probs,\n",
    "    'correct': test_labels == test_preds\n",
    "})\n",
    "\n",
    "predictions_df.to_csv(os.path.join(CONFIG['output_dir'], 'test_predictions.csv'), index=False)\n",
    "print(\"✓ Test predictions saved to test_predictions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8869b13c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T14:33:40.332731Z",
     "iopub.status.busy": "2025-12-19T14:33:40.332243Z",
     "iopub.status.idle": "2025-12-19T14:33:40.355834Z",
     "shell.execute_reply": "2025-12-19T14:33:40.355238Z"
    },
    "papermill": {
     "duration": 2.949678,
     "end_time": "2025-12-19T14:33:40.357031",
     "exception": false,
     "start_time": "2025-12-19T14:33:37.407353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\nExporting model for inference...\")\n",
    "\n",
    "# Save model architecture and weights separately for easier loading\n",
    "torch.save(model.state_dict(), os.path.join(CONFIG['output_dir'], 'model_weights.pth'))\n",
    "\n",
    "# Save complete model info\n",
    "model_info = {\n",
    "    'architecture': 'FaceClassifierCNN',\n",
    "    'input_size': CONFIG['img_size'],\n",
    "    'num_classes': 2,\n",
    "    'total_parameters': total_params,\n",
    "    'trainable_parameters': trainable_params,\n",
    "    'best_val_accuracy': best_val_acc,\n",
    "    'test_accuracy': accuracy * 100,\n",
    "    'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'config': CONFIG\n",
    "}\n",
    "\n",
    "with open(os.path.join(CONFIG['output_dir'], 'model_info.json'), 'w') as f:\n",
    "    json.dump(model_info, f, indent=2)\n",
    "\n",
    "print(\"✓ Model weights saved to model_weights.pth\")\n",
    "print(\"✓ Model info saved to model_info.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5694f435",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T14:33:46.293834Z",
     "iopub.status.busy": "2025-12-19T14:33:46.293550Z",
     "iopub.status.idle": "2025-12-19T14:33:46.298831Z",
     "shell.execute_reply": "2025-12-19T14:33:46.298169Z"
    },
    "papermill": {
     "duration": 2.931676,
     "end_time": "2025-12-19T14:33:46.300039",
     "exception": false,
     "start_time": "2025-12-19T14:33:43.368363",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL TASKS COMPLETED SUCCESSFULLY! 🎉\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nFinal Test Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"Total Training Time: {total_time/3600:.2f} hours\")\n",
    "print(f\"\\nAll outputs saved to: {CONFIG['output_dir']}\")\n",
    "print(\"\\nYou can now:\")\n",
    "print(\"  1. Review the learning curves and metrics visualizations\")\n",
    "print(\"  2. Load the best model for inference\")\n",
    "print(\"  3. Analyze the test predictions\")\n",
    "print(\"  4. Fine-tune hyperparameters based on the results\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 501529,
     "sourceId": 939937,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 42018.41391,
   "end_time": "2025-12-19T14:33:52.392973",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-19T02:53:33.979063",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
